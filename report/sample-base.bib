@book{grooten2018living,
  title={Living planet report-2018: aiming higher.},
  author={Grooten, Monique and Almond, Rosamunde EA and others},
  year={2018},
  publisher={WWF international}
}

@InProceedings{Redmon_2016_CVPR,
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
title = {You Only Look Once: Unified, Real-Time Object Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages=10,
publisher= {Conference on Computer Vision and Pattern Recognition (CVPR)},
address={Las Vegas},
month = {June},
year = {2016}
}

@article{naqvi2022camera,
  title={Camera traps are an effective tool for monitoring insect--plant interactions},
  author={Naqvi, Qaim and Wolff, Patrick J and Molano-Flores, Brenda and Sperry, Jinelle H},
  journal={Ecology and Evolution},
  volume={12},
  number={6},
  pages={e8962},
  year={2022},
  publisher={Wiley Online Library}
}

@article{ratnayake2021tracking,
    doi = {10.1371/journal.pone.0239504},
    author = {Ratnayake, Malika Nisal AND Dyer, Adrian G. AND Dorin, Alan},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Tracking individual honeybees among wildflower clusters with computer vision-facilitated pollinator monitoring},
    year = {2021},
    month = {02},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pone.0239504},
    pages = {1-20},
    abstract = {Monitoring animals in their natural habitat is essential for advancement of animal behavioural studies, especially in pollination studies. Non-invasive techniques are preferred for these purposes as they reduce opportunities for research apparatus to interfere with behaviour. One potentially valuable approach is image-based tracking. However, the complexity of tracking unmarked wild animals using video is challenging in uncontrolled outdoor environments. Out-of-the-box algorithms currently present several problems in this context that can compromise accuracy, especially in cases of occlusion in a 3D environment. To address the issue, we present a novel hybrid detection and tracking algorithm to monitor unmarked insects outdoors. Our software can detect an insect, identify when a tracked insect becomes occluded from view and when it re-emerges, determine when an insect exits the camera field of view, and our software assembles a series of insect locations into a coherent trajectory. The insect detecting component of the software uses background subtraction and deep learning-based detection together to accurately and efficiently locate the insect among a cluster of wildflowers. We applied our method to track honeybees foraging outdoors using a new dataset that includes complex background detail, wind-blown foliage, and insects moving into and out of occlusion beneath leaves and among three-dimensional plant structures. We evaluated our software against human observations and previous techniques. It tracked honeybees at a rate of 86.6% on our dataset, 43% higher than the computationally more expensive, standalone deep learning model YOLOv2. We illustrate the value of our approach to quantify fine-scale foraging of honeybees. The ability to track unmarked insect pollinators in this way will help researchers better understand pollination ecology. The increased efficiency of our hybrid approach paves the way for the application of deep learning-based techniques to animal tracking in real-time using low-powered devices suitable for continuous monitoring.},
    number = {2},

}
@inproceedings{ratnayake2021towards,
  title={Towards computer vision and deep learning facilitated pollination monitoring for agriculture},
  author={Ratnayake, Malika Nisal and Dyer, Adrian G and Dorin, Alan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  publisher={Conference on computer vision and pattern recognition},
  address={Nashville},
  pages={2921--2930},
  year={2021}
}

@misc{wang2024yolov9,
      title={YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information}, 
      author={Chien-Yao Wang and I-Hau Yeh and Hong-Yuan Mark Liao},
      year={2024},
      eprint={2402.13616},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{fukushima1980neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  author={Fukushima, Kunihiko},
  journal={Biological cybernetics},
  volume={36},
  number={4},
  pages={193--202},
  year={1980},
  publisher={Springer}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
pages={9},
  volume={25},
  year={2012}
}

@inproceedings{wang2023yolov7,
  title={YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={7464--7475},
  publisher={Conference on computer vision and pattern recognition},
  address={Vancouver},
  year={2023}
}

@article{ollerton2011many,
  title={How many flowering plants are pollinated by animals?},
  author={Ollerton, Jeff and Winfree, Rachael and Tarrant, Sam},
  journal={Oikos},
  volume={120},
  number={3},
  pages={321--326},
  year={2011},
  publisher={Wiley Online Library}
}
@InProceedings{Peng_2018_CVPR,
author = {Peng, Chao and Xiao, Tete and Li, Zeming and Jiang, Yuning and Zhang, Xiangyu and Jia, Kai and Yu, Gang and Sun, Jian},
title = {MegDet: A Large Mini-Batch Object Detector},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
publisher={Conference on Computer Vision and Pattern Recognition},
address={Salt Lake City},
pages=9,
month = {June},
year = {2018}
}
@article{gallai2009economic,
  title={Economic valuation of the vulnerability of world agriculture confronted with pollinator decline},
  author={Gallai, Nicola and Salles, Jean-Michel and Settele, Josef and Vaissi{\`e}re, Bernard E},
  journal={Ecological economics},
  volume={68},
  number={3},
  pages={810--821},
  year={2009},
  publisher={Elsevier}
}

@article{eilers2011contribution,
  title={Contribution of pollinator-mediated crops to nutrients in the human food supply},
  author={Eilers, Elisabeth J and Kremen, Claire and Smith Greenleaf, Sarah and Garber, Andrea K and Klein, Alexandra-Maria},
  journal={PLoS one},
  volume={6},
  number={6},
  pages={e21363},
  year={2011},
  publisher={Public Library of Science San Francisco, USA}
}

@article{sanchez2019worldwide,
  title={Worldwide decline of the entomofauna: A review of its drivers},
  author={S{\'a}nchez-Bayo, Francisco and Wyckhuys, Kris AG},
  journal={Biological conservation},
  volume={232},
  pages={8--27},
  year={2019},
  publisher={Elsevier}
}
@article{CHERNOV2015328,
title = {Integer-based accurate conversion between RGB and HSV color spaces},
journal = {Computers \& Electrical Engineering},
volume = {46},
pages = {328-337},
year = {2015},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0045790615002827},
author = {Vladimir Chernov and Jarmo Alander and Vladimir Bochko},
keywords = {HSV, RGB, Integer algorithm, Color space conversion},
abstract = {This paper introduces a new fast integer-based algorithm to convert the RGB color representation to HSV and vice versa. The proposed algorithm is as accurate as the classical real-valued one. The use of only integer operations increases performance and portability. Performance measurement results show a speed gain of about two times when compared with the classical C++ language implementation on PC platforms. Lookup tables are not involved, thus the memory usage is minimal. The resulting HSV color can be packed into 48 bits. The proposed method can safely replace the commonly used floating-point implementation.}
}

@InProceedings{Gebauer_2024_WACV,
    author    = {Gebauer, Eike and Thiele, Sebastian and Ouvrard, Pierre and Sicard, Adrien and Risse, Benjamin},
    title     = {Towards a Dynamic Vision Sensor-Based Insect Camera Trap},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    publisher= {Winter Conference on Applications of Computer Vision},
    address = {Waikoloa},
    year      = {2024},
    pages     = {7157-7166}
}
@article{bradski2000opencv,
  title={The openCV library.},
  author={Bradski, Gary},
  journal={Dr. Dobb's Journal: Software Tools for the Professional Programmer},
  volume={25},
  number={11},
  pages={120--123},
  year={2000},
  publisher={Miller Freeman Inc.}
}

@article{zhang2000flexible,
  title={A flexible new technique for camera calibration},
  author={Zhang, Zhengyou},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={22},
  number={11},
  pages={1330--1334},
  year={2000},
  publisher={IEEE}
}
@article{glasbey1998review,
  title={A review of image-warping methods},
  author={Glasbey, Chris A and Mardia, Kantilal Vardichand},
  journal={Journal of applied statistics},
  volume={25},
  number={2},
  pages={155--171},
  year={1998},
  publisher={Taylor \& Francis}
}

@unpublished{thiele2021towards,
    author = {Thiele, Sebastian and Haalck, Lars and Struffert, Marvin and Scherber, Christoph and Risse, Benjamin },
    title = {Towards Visual Insect Camera Traps},
    journal = {International Conference on
Pattern Recognition (ICPR) Visual observation and analysis of Vertebrate And Insect Behavior Workshop},
    year = 2021,
pages=4
}
@inproceedings{delbruck2016neuromorophic,
  title={Neuromorophic vision sensing and processing},
  author={Delbruck, Tobi},
  booktitle={2016 46Th european solid-state device research conference (ESSDERC)},
  pages={7--14},
  year={2016},
  publisher={European Solid-State Device Research Conference},
address={Lausanne},
  organization={IEEE}
}

@unpublished{scharf2023TEMP,
  author = {Scharf, Paula},
  title  = {Monitoring of insects in the wild: Investigating the feasibility of machine learning based tiny object detection in time-lapse video recordings},
  note = {"Master Thesis"},
  month = 3,
  year = {2023},
 annote = {(Unpublished)}
}
@article{aghdam2017guide,
  title={Guide to convolutional neural networks},
  author={Aghdam, Hamed Habibi and Heravi, Elnaz Jahani and others},
  journal={New York, NY: Springer},
  volume={10},
  number={978-973},
  pages={51},
  year={2017},
  publisher={Springer}
}
@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  pages={303--338},
  year={2010},
  publisher={Springer}
}
@book{szeliski2022computer,
  title={Computer vision: algorithms and applications},
  author={Szeliski, Richard},
  year={2022},
  publisher={Springer Nature},
  address={London},
}

@article{dietrich1998approximate,
    author = {Dietterich, Thomas G.},
    title = "{Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms}",
    journal = {Neural Computation},
    volume = {10},
    number = {7},
    pages = {1895-1923},
    year = {1998},
    month = {10},
    abstract = "{This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5×2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.}",
    issn = {0899-7667},
    doi = {10.1162/089976698300017197},
    url = {https://doi.org/10.1162/089976698300017197},
    eprint = {https://direct.mit.edu/neco/article-pdf/10/7/1895/814002/089976698300017197.pdf},
}

@article{raschka2018model,
  author       = {Sebastian Raschka},
  title        = {Model Evaluation, Model Selection, and Algorithm Selection in Machine
                  Learning},
  journal      = {CoRR},
  volume       = {abs/1811.12808},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.12808},
  eprinttype    = {arXiv},
  eprint       = {1811.12808},
  timestamp    = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-12808.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pages=49,
}
